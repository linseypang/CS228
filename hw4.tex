\def\s{\section*}
\def\ss{\subsection*}
\def\sp{\hspace{3 mm}}
\def\ind{\!\perp\!\!\!\perp}
\def\m{\mid}
\def\nl{ \\ = \sp &}
\newcommand{\eq}[1]{\begin{align*}&{#1}\end{align*}}
\renewcommand{\ss}[1]{\subsection*{#1}}
\renewcommand{\P}[1]{\s{Problem #1}}
\def\ra{\rightarrow}
\def\la{\leftarrow}
\def \pb{\newline\newline}
\newcommand{\pic}[2]{\begin{figure}[H]
  \includegraphics[width=\linewidth]{#2}
  \caption{#1}
  \label{fig:net}
\end{figure}}

\documentclass[10pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{parskip}
\usepackage{amsfonts}
\usepackage{lmodern}
\usepackage{physics}
\usepackage{float}
\usepackage{graphicx}
\usepackage{amssymb}
\author{Hugh Zhang}
\title{CS 228 Problem Set 1}
\date{\today}
\begin{document}
\maketitle

\P 1

We want to compute $P(C \mid v_1\dots v_k)$. For our purposes, our sampler Q is uniform on the possible, so assuming a permutation x' is possible to reach from a state x, then the probabilities are equal. Thus, we can cancel the Qs out.

Acceptance probability is

\begin{align*}
& A(c' \mid c, v_1 \dots v_k) \nl min (1, \frac{P(c' \mid v_1 \dots v_k )Q(c \mid x', v_1 \dots v_k)}{P(c \mid v_1 \dots v_k )Q(c' \mid x, v_1 \dots v_k}) \nl
min (1, \frac{P(c' \mid v_1 \dots v_k )}{P(c \mid v_1 \dots v_k )})
\end{align*}

How do we calculate \[
\frac{P(c' \mid v_1 \dots v_k)}{P(c \mid v_1 \dots v_k)}
\]

\begin{align*}
& \frac{P(c' \mid v_1 \dots v_k)}{P(c \mid v_1 \dots v_k)} \nl
\frac{P(c', v_1 \dots v_k)}{P(c, v_1 \dots v_k)} \nl
\frac{P(v_1 \dots v_k \mid c') P(c')}{P(v_1 \dots v_k \mid c) P(c)}
\end{align*}

We are given $P(v_1 \dots v_k \mid C)$, and although we don't quite know P(c), we are given the assumption that it is uniform a priori, so for our approximation it cancel out.

\ss {1.2}

The samples are directly from $P(C \mid v_1 \dots v_k)$. Thus,
\[P(C_i = k \mid v_1 \dots v_k) = \frac{\sum_{m=1}^M 1(C_i[m]==k)}{M}
\], or in English, just count the number of times you see it in the sample and divide it by the total number of samples.

\ss {1.3}

Gibbs sampling does not work. When we sample C, we take two elements $C_i$ and $C_j$ and swap them. If we were to try to Gibbs sample this and try to sample each $C_i$ independently and in order for all i, we would get invalid samples that were not permutations.

\P 2

From lecture, we have.

\begin{align*}
& \log P(y_i \mid x_i, \theta) \nl \log(\frac{1}{Z(x^i, \theta)} \prod_{n \in N} exp(\theta_n f_n(x^i,y_n^i)))) \nl
\sum_{n \in N} \theta_n f_n(x^i,y_n^i)) - log(Z(x^i, \theta)) \nl
\sum_{n \in N} \theta_n f_n(x^i,y_n^i)) - log \sum_n \sum_y exp(\theta_n * f_n(y,x^i))
\end{align*}

Since the likelihood is just the log of the probability summed over all the data, letting M be the number of examples in D, we have

\begin{align*}
& g(\theta, D) = (1-\alpha)\ell_{Y \mid X}(\theta, D) + \alpha\ell_{X \mid Y}(\theta,D)
\end{align*}

Where as per above,

\begin{align*}
&\ell_{Y \mid X}(\theta, D) = \sum_i^M (\sum_{n \in N} \theta_n f_n(x_n^i,y_n^i)) - log \sum_n \sum_y exp(\theta_n * f_n(y_n,x_n^i))) \\ \sp &
\ell_{X \mid Y}(\theta, D) = \sum_i^M (\sum_{n \in N} \theta_n f_n(x_n^i,y_n^i)) - log \sum_n \sum_x exp(\theta_n * f_n(y_n^i,x_n)))
\end{align*}

\ss{2.2}

We want to calculate 

\def\t{\pdv{\theta}}

\begin{align*}
& \pdv{\theta} g(\theta,D) \nl
(1-\alpha)\t\ell_{Y \mid X}(\theta, D) + \alpha\t\ell_{X \mid Y}(\theta,D)\nl
\end{align*}

We calculate 
\begin{align*}
& \t\ell_{Y \mid X}(\theta, D) \nl
\t \sum_i^M (\sum_{n \in N} \theta_n f_n(x_n^i,y_n^i)) - log \sum_n \sum_y exp(\theta_n * f_n(y_n,x_n^i))) \nl
\sum_i^M (\sum_{n \in N} f_n(x_n^i,y_n^i)) - \t\log \sum_n \sum_y exp(\theta_n * f_n(y_n,x_n^i))) \nl
\sum_i^M (\sum_{n \in N} f_n(x_n^i,y_n^i)) - \frac{1}{\sum_n \sum_y exp(\theta_n * f_n(y_n,x_n^i)))} * \t (\sum_n \sum_y exp(\theta_n * f_n(y_n,x_n^i)))) \nl
\sum_i^M (\sum_{n \in N} f_n(x_n^i,y_n^i)) - \frac{\sum_n \sum_y exp(\theta_n * f_n(y_n,x_n^i)) * f_n(y_n,x_n^i))}{\sum_n \sum_y exp(\theta_n * f_n(y_n,x_n^i)))} \nl
\sum_i^M (\sum_{n \in N} f_n(x_n^i,y_n^i)) - \frac{\sum_n P(x_n^i) * f_n(y_n,x_n^i))}{Z(\theta, x_n^i)} \nl
\end{align*}

\end{document}