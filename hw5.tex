\def\s{\section*}
\def\ss{\subsection*}
\def\sp{\hspace{3 mm}}
\def\ind{\!\perp\!\!\!\perp}
\def\m{\mid}
\def\nl{ \\ = \sp &}
\newcommand{\eq}[1]{\begin{align*}&{#1}\end{align*}}
\renewcommand{\ss}[1]{\subsection*{#1}}
\renewcommand{\P}[1]{\s{Problem #1}}
\def\ra{\rightarrow}
\def\la{\leftarrow}
\def \pb{\newline\newline}
\newcommand{\pic}[2]{\begin{figure}[H]
  \includegraphics[width=\linewidth]{#2}
  \caption{#1}
  \label{fig:net}
\end{figure}}

\documentclass[10pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{parskip}
\usepackage{amsfonts}
\usepackage{lmodern}
\usepackage{physics}
\usepackage{float}
\usepackage{graphicx}
\usepackage{amssymb}
\author{Hugh Zhang}
\title{CS 228 Problem Set 5}
\date{\today}
\begin{document}
\maketitle

\P 1

Note that from the hint, we can see that the Dirichlet distribution is a distributed beta, so all the lecture theorems apply.

\begin{align*}
& P(X[M+1] = x^i \mid D) \nl
\sum_{\theta}P(X[M+1] = x^i \mid \theta) * P(\theta \mid D) \nl
\sum_{\theta} \theta_i* P(\theta \mid D) \nl
E[\theta_i \mid D] = \frac{\alpha_i + M[i]}{M + \alpha}
\end{align*}

As proven in lecture. TODO GO TO OFFICE HOURS

\ss{1.2}

\begin{align*}
& P(X[M+1] = x^i, X[M+2] = x^j \mid D) \nl
P(X[M+1] = x^i \mid D) * P(X[M+2] = x^i \mid  X[M+1] = x^j, D)
\end{align*}

\ss{1.3}

\begin{align*}
& P(X[M+1] = x^i, X[M+2] = x^j \mid D) \\ & - P(X[M+1] = x^i \mid D) * P(X[M+2] = x^i \mid  D) \nl
\frac{\alpha_i + M[i]}{M + \alpha} * \frac{\alpha_j + M[j] + 1(i=j)}{M + \alpha + 1} - \frac{\alpha_i + M[i]}{M + \alpha} * \frac{\alpha_j + M[j]}{M + \alpha} \nl
\frac{\alpha_i + M[i]}{M + \alpha} (\frac{\alpha_j + M[j] + 1(i=j)}{M + \alpha + 1} - \frac{\alpha_j + M[j]}{M + \alpha}) \nl
\frac{\alpha_i + M[i]}{M + \alpha} (\frac{(M+\alpha) \alpha_j + (M+\alpha)*M[j] + (M+\alpha)*1(i=j)}{(M + \alpha + 1)*(M+\alpha)} - \frac{(M + \alpha + 1)\alpha_j + (M + \alpha + 1)*M[j]}{(M + \alpha)(M + \alpha + 1)}) \nl
\frac{\alpha_i + M[i]}{M + \alpha} (\frac{  -\alpha_j - M[j] + (M+\alpha)*1(i=j)}{(M + \alpha + 1)*(M+\alpha)}) \nl
\end{align*}

The difference here is if you update your Dirichlet counts after you sample the M+1th sample when you do exact inference and don't if you do approximate. Thus, if your data set is large, then not updating doesn't matter, but if it is small it might make a large difference.

The numerator can at most asymptotically approach $M^2$, while the denominator is assymptotically $M^3$, thus the error asymptotically decreases linearly as M increases.

\end{document}